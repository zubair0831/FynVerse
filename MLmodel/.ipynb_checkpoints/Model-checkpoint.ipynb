{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "474ebe4b-49fc-402f-800e-e0dd0b4ef450",
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "\nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <8E129FE8-EF1C-38EA-A9CF-202782564052> /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file)\"]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mXGBoostError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomizedSearchCV, TimeSeriesSplit\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LGBMClassifier\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"XGBoost: eXtreme Gradient Boosting library.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mContributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracker  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     Booster,\n\u001b[32m     10\u001b[39m     DataIter,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     build_info,\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/tracker.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IntEnum, unique\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, Union\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LIB, _check_call, _deprecate_positional_args, make_jcargs\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_family\u001b[39m(addr: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m     13\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get network family from address.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/core.py:295\u001b[39m\n\u001b[32m    291\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\n\u001b[32m    294\u001b[39m \u001b[38;5;66;03m# load the XGBoost library globally\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m _LIB = \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_call\u001b[39m(ret: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    299\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[32m    300\u001b[39m \n\u001b[32m    301\u001b[39m \u001b[33;03m    This function will raise exception when error occurs.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m \u001b[33;03m        return value from API calls\u001b[39;00m\n\u001b[32m    308\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/core.py:257\u001b[39m, in \u001b[36m_load_lib\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib_success:\n\u001b[32m    256\u001b[39m         libname = os.path.basename(lib_paths[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(\n\u001b[32m    258\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[33mXGBoost Library (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlibname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) could not be loaded.\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[33mLikely causes:\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[33m  * OpenMP runtime is not installed\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[33m    - vcomp140.dll or libgomp-1.dll for Windows\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[33m    - libomp.dylib for Mac OSX\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[33m    - libgomp.so for Linux and other UNIX-like OSes\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[33m    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\u001b[39m\n\u001b[32m    266\u001b[39m \n\u001b[32m    267\u001b[39m \u001b[33m  * You are running 32-bit Python on a 64-bit OS\u001b[39m\n\u001b[32m    268\u001b[39m \n\u001b[32m    269\u001b[39m \u001b[33mError message(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_error_list\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    271\u001b[39m         )\n\u001b[32m    272\u001b[39m     _register_log_callback(lib)\n\u001b[32m    274\u001b[39m     libver = _lib_version(lib)\n",
      "\u001b[31mXGBoostError\u001b[39m: \nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <8E129FE8-EF1C-38EA-A9CF-202782564052> /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file)\"]\n"
     ]
    }
   ],
   "source": [
    "# nifty_ml_pipeline.py\n",
    "\"\"\"\n",
    "NIFTY50 ML pipeline for 3 horizons + crash risk.\n",
    "Outputs per-horizon probability scores in [0,1].\n",
    "Requirements:\n",
    "pip install yfinance pandas numpy ta scikit-learn xgboost lightgbm joblib imbalanced-learn matplotlib\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import timedelta\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Optional: pip install ta\n",
    "try:\n",
    "    import ta\n",
    "except Exception:\n",
    "    raise Exception(\"Install 'ta' package: pip install ta\")\n",
    "\n",
    "# ------------ CONFIG ------------\n",
    "# Hardcoded Nifty50 tickers (yfinance style) - tweak if needed.\n",
    "NIFTY50 = [\n",
    "\"RELIANCE.NS\",\"TCS.NS\",\"HDFCBANK.NS\",\"INFY.NS\",\"HDFC.NS\",\"HINDUNILVR.NS\",\"ICICIBANK.NS\",\"KOTAKBANK.NS\",\n",
    "\"SBIN.NS\",\"BHARTIARTL.NS\",\"ITC.NS\",\"LT.NS\",\"AXISBANK.NS\",\"ASIANPAINT.NS\",\"HCLTECH.NS\",\"BAJAJ-AUTO.NS\",\n",
    "\"ULTRACEMCO.NS\",\"SUNPHARMA.NS\",\"POWERGRID.NS\",\"NTPC.NS\",\"TITAN.NS\",\"EICHERMOT.NS\",\"MARUTI.NS\",\"ONGC.NS\",\n",
    "\"NESTLEIND.NS\",\"TECHM.NS\",\"TATASTEEL.NS\",\"BRITANNIA.NS\",\"HINDALCO.NS\",\"DIVISLAB.NS\",\"ADANIENT.NS\",\"M&M.NS\",\n",
    "\"BPCL.NS\",\"GRASIM.NS\",\"INDUSINDBK.NS\",\"WIPRO.NS\",\"COALINDIA.NS\",\"SHREECEM.NS\",\"HDFCLIFE.NS\",\"CIPLA.NS\",\n",
    "\"IOC.NS\",\"JSWSTEEL.NS\",\"BAJAJFINSV.NS\",\"BRIDGESTONE.NS\",\"SBILIFE.NS\"  # add/remove to reach full 50 as necessary\n",
    "]\n",
    "# If list incomplete, add remainder.\n",
    "\n",
    "DATA_DIR = \"./data_nifty\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "MODEL_DIR = \"./models_nifty\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Horizons in trading days approximations:\n",
    "HORIZONS = {\n",
    "    \"very_short\": 5,       # ~1 week (~5 trading days)\n",
    "    \"short\": 90,           # ~3 months (use 63-126 range — pick 90 for training target)\n",
    "    \"long\": 520            # ~2 years (approx 2*252 trading days)\n",
    "}\n",
    "# Target return thresholds (tunable)\n",
    "RETURN_THRESHOLDS = {\n",
    "    \"very_short\": 0.015,   # 1.5% in 1 week\n",
    "    \"short\": 0.15,         # 15% in 3-6 months\n",
    "    \"long\": 0.7            # 70% in 2+ years (you can lower to 0.5)\n",
    "}\n",
    "# Crash definition for risk model: drop >= this within horizon\n",
    "CRASH_DROP = 0.30  # 30% drawdown\n",
    "\n",
    "# ---------------- Utility functions ----------------\n",
    "def fetch_price_and_info(ticker, period=\"max\"):\n",
    "    \"\"\"Download OHLCV and info using yfinance. Returns dataframe and info dict\"\"\"\n",
    "    t = yf.Ticker(ticker)\n",
    "    df = t.history(period=period, actions=False)\n",
    "    if df.empty:\n",
    "        print(f\"Warning: {ticker} returned empty history.\")\n",
    "        return None, {}\n",
    "    # unify column names if necessary\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    info = {}\n",
    "    try:\n",
    "        info = t.info\n",
    "    except Exception:\n",
    "        info = {}\n",
    "    return df, info\n",
    "\n",
    "def compute_technical_indicators(df):\n",
    "    \"\"\"Add technical indicators using ta\"\"\"\n",
    "    df = df.copy()\n",
    "    # require close, high, low, volume\n",
    "    df[\"return_1d\"] = df[\"Close\"].pct_change()\n",
    "    # Moving averages\n",
    "    for w in [5, 10, 20, 50, 100, 200]:\n",
    "        df[f\"sma_{w}\"] = df[\"Close\"].rolling(w).mean()\n",
    "        df[f\"ema_{w}\"] = df[\"Close\"].ewm(span=w, adjust=False).mean()\n",
    "    # RSI\n",
    "    df[\"rsi_14\"] = ta.momentum.RSIIndicator(df[\"Close\"], window=14, fillna=True).rsi()\n",
    "    # MACD\n",
    "    macd = ta.trend.MACD(df[\"Close\"])\n",
    "    df[\"macd\"] = macd.macd()\n",
    "    df[\"macd_signal\"] = macd.macd_signal()\n",
    "    # Bollinger band width\n",
    "    bb = ta.volatility.BollingerBands(close=df[\"Close\"], window=20)\n",
    "    df[\"bb_width\"] = (bb.bollinger_hband() - bb.bollinger_lband()) / df[\"Close\"]\n",
    "    # ATR\n",
    "    df[\"atr_14\"] = ta.volatility.AverageTrueRange(df[\"High\"], df[\"Low\"], df[\"Close\"], window=14).average_true_range()\n",
    "    # Volume ratios\n",
    "    for w in [5, 20, 60, 200]:\n",
    "        df[f\"vol_sma_{w}\"] = df[\"Volume\"].rolling(w).mean()\n",
    "        df[f\"vol_ratio_{w}\"] = df[\"Volume\"] / (df[f\"vol_sma_{w}\"] + 1e-9)\n",
    "    # Price-based momentum\n",
    "    df[\"mom_10\"] = df[\"Close\"].pct_change(10)\n",
    "    df[\"mom_21\"] = df[\"Close\"].pct_change(21)\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "def extract_fundamentals(info):\n",
    "    \"\"\"Extract a few fundamentals from yfinance info dict (best-effort).\"\"\"\n",
    "    # This is messy: different tickers may have different fields; handle gracefully\n",
    "    o = {}\n",
    "    # Market metrics\n",
    "    o[\"marketCap\"] = info.get(\"marketCap\", np.nan)\n",
    "    o[\"trailingPE\"] = info.get(\"trailingPE\", np.nan)\n",
    "    o[\"forwardPE\"] = info.get(\"forwardPE\", np.nan)\n",
    "    o[\"priceToBook\"] = info.get(\"priceToBook\", np.nan)\n",
    "    o[\"pegRatio\"] = info.get(\"pegRatio\", np.nan)\n",
    "    o[\"trailingEps\"] = info.get(\"trailingEps\", np.nan)\n",
    "    o[\"earningsQuarterlyGrowth\"] = info.get(\"earningsQuarterlyGrowth\", np.nan)\n",
    "    o[\"revenueGrowth\"] = info.get(\"revenueGrowth\", np.nan)\n",
    "    # Balance-sheet derived metrics might need financials DataFrame; keep null if not present\n",
    "    return o\n",
    "\n",
    "def build_features_for_ticker(ticker, period=\"max\"):\n",
    "    \"\"\"Return a DataFrame with engineered features and fundamentals merged per date\"\"\"\n",
    "    df, info = fetch_price_and_info(ticker, period=period)\n",
    "    if df is None:\n",
    "        return None\n",
    "    df = compute_technical_indicators(df)\n",
    "    fund = extract_fundamentals(info)\n",
    "    # Broadcast fundamentals as constant columns\n",
    "    for k,v in fund.items():\n",
    "        df[f\"f_{k}\"] = v if (v is not None) else np.nan\n",
    "    # also add ticker column for later grouping\n",
    "    df[\"ticker\"] = ticker\n",
    "    df = df.dropna(how=\"all\")  # drop rows if entire row NaN\n",
    "    return df\n",
    "\n",
    "# --------------- Labeling ----------------\n",
    "def make_labels(df, horizons=HORIZONS, thresholds=RETURN_THRESHOLDS, crash_drop=CRASH_DROP):\n",
    "    \"\"\"\n",
    "    For each horizon create:\n",
    "    - binary label: 1 if future_return >= threshold, else 0\n",
    "    - crash label: 1 if max drawdown in future horizon >= crash_drop, else 0\n",
    "    Returns df with new label columns.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for name, days in horizons.items():\n",
    "        future_close = df[\"Close\"].shift(-days)\n",
    "        future_return = (future_close - df[\"Close\"]) / df[\"Close\"]\n",
    "        df[f\"label_{name}\"] = (future_return >= thresholds[name]).astype(int)\n",
    "        # crash: compute rolling future max drop: easiest way is to compute max future price drop relative to current\n",
    "        # naive implementation: within next `days`, compute min(close) and compare:\n",
    "        df[f\"min_future_{name}\"] = df[\"Close\"].shift(-1).rolling(window=days, min_periods=1).min().shift(-(days-1))\n",
    "        # The above may give misalignment at tail; simpler explicit loop for correctness:\n",
    "        min_list = []\n",
    "        closes = df[\"Close\"].values\n",
    "        n = len(closes)\n",
    "        for i in range(n):\n",
    "            j = min(n, i + days + 1)\n",
    "            future_min = np.min(closes[i+1:j]) if (i+1 < j) else closes[i]\n",
    "            min_list.append(future_min)\n",
    "        df[f\"min_future_price_{name}\"] = min_list\n",
    "        df[f\"future_max_drawdown_{name}\"] = (df[\"Close\"] - df[f\"min_future_price_{name}\"]) / df[\"Close\"]\n",
    "        df[f\"crash_{name}\"] = (df[f\"future_max_drawdown_{name}\"] >= crash_drop).astype(int)\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    return df\n",
    "\n",
    "# --------------- Aggregation across tickers ---------------\n",
    "def build_dataset(tickers=NIFTY50, period=\"max\", save_intermediate=True):\n",
    "    \"\"\"Download data for all tickers, build features, labels, and return concatenated DataFrame\"\"\"\n",
    "    all_dfs = []\n",
    "    for t in tickers:\n",
    "        print(\"Fetching:\", t)\n",
    "        try:\n",
    "            df = build_features_for_ticker(t, period=period)\n",
    "        except Exception as e:\n",
    "            print(\"Error for\", t, e)\n",
    "            df = None\n",
    "        if df is None:\n",
    "            continue\n",
    "        df = make_labels(df)\n",
    "        # drop rows near the end where labels are invalid (due to shift)\n",
    "        df = df.dropna(subset=[f\"label_very_short\", f\"label_short\", f\"label_long\"])\n",
    "        all_dfs.append(df)\n",
    "        if save_intermediate:\n",
    "            df.to_parquet(os.path.join(DATA_DIR, f\"{t.replace('/', '_')}.parquet\"))\n",
    "    if not all_dfs:\n",
    "        raise RuntimeError(\"No data fetched.\")\n",
    "    full = pd.concat(all_dfs)\n",
    "    # Ensure sorted by date\n",
    "    full = full.sort_index()\n",
    "    return full\n",
    "\n",
    "# ---------------- Model training and backtest ----------------\n",
    "def select_feature_columns(df):\n",
    "    \"\"\"Pick which columns to use as predictors (exclude unwanted ones)\"\"\"\n",
    "    # all columns that start with sma_, ema_, rsi_, macd, bb_, atr_, vol_ratio_, mom_, f_ (fundamentals)\n",
    "    features = [c for c in df.columns if (\n",
    "        c.startswith(\"sma_\") or c.startswith(\"ema_\") or c.startswith(\"rsi_\")\n",
    "        or c.startswith(\"macd\") or c.startswith(\"bb_\") or c.startswith(\"atr_\")\n",
    "        or c.startswith(\"vol_ratio_\") or c.startswith(\"mom_\") or c.startswith(\"return_\")\n",
    "        or c.startswith(\"f_\") or c.startswith(\"Close_Ratio\") or c.startswith(\"vol_sma_\")\n",
    "    )]\n",
    "    # Also you may add price level features:\n",
    "    features += [\"Close\", \"Volume\"]\n",
    "    features = [f for f in features if f in df.columns]\n",
    "    return features\n",
    "\n",
    "def train_single_model(X, y, model_type=\"xgb\", n_iter=20):\n",
    "    \"\"\"Train a single classifier with RandomizedSearchCV (time-series split)\"\"\"\n",
    "    if model_type == \"xgb\":\n",
    "        base = XGBClassifier(objective=\"binary:logistic\", use_label_encoder=False, eval_metric=\"logloss\", n_jobs=4)\n",
    "        param_dist = {\n",
    "            \"n_estimators\": [100, 300, 500],\n",
    "            \"max_depth\": [3, 5, 7, 9],\n",
    "            \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "            \"subsample\": [0.6, 0.8, 1.0],\n",
    "            \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "            \"reg_alpha\": [0, 0.1, 1],\n",
    "            \"reg_lambda\": [1, 5, 10]\n",
    "        }\n",
    "    else:\n",
    "        base = LGBMClassifier(n_jobs=4)\n",
    "        param_dist = {\n",
    "            \"n_estimators\": [100, 300, 500],\n",
    "            \"max_depth\": [ -1, 5, 10, 20],\n",
    "            \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "            \"num_leaves\": [31, 50, 100],\n",
    "            \"subsample\": [0.6, 0.8, 1.0]\n",
    "        }\n",
    "    # time series split\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    rs = RandomizedSearchCV(base, param_dist, n_iter=n_iter, cv=tscv, scoring=\"precision\", verbose=1, n_jobs=4, random_state=42)\n",
    "    rs.fit(X, y)\n",
    "    print(\"Best params:\", rs.best_params_)\n",
    "    return rs.best_estimator_\n",
    "\n",
    "def walk_forward_backtest(df, feature_cols, label_col, model_type=\"xgb\", initial_train_days=252*3, step_days=63):\n",
    "    \"\"\"\n",
    "    Walk-forward backtest. initial_train_days: how many days to start training.\n",
    "    step_days: length of test window each iteration.\n",
    "    Returns predictions DataFrame with columns: y_true, y_pred, prob.\n",
    "    \"\"\"\n",
    "    df = df.sort_index()\n",
    "    unique_dates = sorted(df.index.unique())\n",
    "    results = []\n",
    "    i = initial_train_days\n",
    "    n = len(unique_dates)\n",
    "    while i < n - 5:\n",
    "        train_dates = unique_dates[:i]\n",
    "        test_dates = unique_dates[i:i+step_days]\n",
    "        train_df = df.loc[train_dates]\n",
    "        test_df = df.loc[test_dates]\n",
    "        # use all tickers in train and test (multi-stock)\n",
    "        # Drop rows with missing label\n",
    "        train_df = train_df.dropna(subset=[label_col])\n",
    "        test_df = test_df.dropna(subset=[label_col])\n",
    "        # If not enough rows, break\n",
    "        if len(train_df) < 200 or len(test_df) == 0:\n",
    "            i += step_days\n",
    "            continue\n",
    "        X_train = train_df[feature_cols]\n",
    "        y_train = train_df[label_col]\n",
    "        X_test = test_df[feature_cols]\n",
    "        y_test = test_df[label_col]\n",
    "        model = train_single_model(X_train, y_train, model_type=model_type, n_iter=10)\n",
    "        prob = model.predict_proba(X_test)[:, 1]\n",
    "        pred = (prob >= 0.5).astype(int)\n",
    "        res = pd.DataFrame({\n",
    "            \"y_true\": y_test,\n",
    "            \"y_pred\": pred,\n",
    "            \"prob\": prob\n",
    "        }, index=test_df.index)\n",
    "        results.append(res)\n",
    "        print(\"Backtest window:\", train_dates[0], \"->\", train_dates[-1], \"| test:\", test_dates[0], \"->\", test_dates[-1], \" | test_rows:\", len(test_df))\n",
    "        i += step_days\n",
    "    if results:\n",
    "        return pd.concat(results)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# --------------- Save/load pipeline ----------------\n",
    "def save_model(model, name):\n",
    "    joblib.dump(model, os.path.join(MODEL_DIR, f\"{name}.joblib\"))\n",
    "\n",
    "def load_model(name):\n",
    "    return joblib.load(os.path.join(MODEL_DIR, f\"{name}.joblib\"))\n",
    "\n",
    "# ---------------- Main flow ----------------\n",
    "def main():\n",
    "    # Step 1: Build dataset (this will fetch data & features)\n",
    "    print(\"Building dataset...\")\n",
    "    df = build_dataset(NIFTY50, period=\"max\", save_intermediate=True)\n",
    "    print(\"Dataset built with rows:\", len(df))\n",
    "    # Step 2: choose feature columns\n",
    "    feature_cols = select_feature_columns(df)\n",
    "    print(\"Feature count:\", len(feature_cols))\n",
    "    # Step 3: Train & backtest for each horizon label\n",
    "    results_summary = {}\n",
    "    for horizon in [\"very_short\", \"short\", \"long\"]:\n",
    "        label_col = f\"label_{horizon}\"\n",
    "        print(\"====== HOR: \", horizon, \"label:\", label_col)\n",
    "        backtest_res = walk_forward_backtest(df, feature_cols, label_col, model_type=\"xgb\",\n",
    "                                            initial_train_days=252*2, step_days=63)\n",
    "        if backtest_res.empty:\n",
    "            print(\"No backtest results for\", horizon)\n",
    "            continue\n",
    "        # metrics\n",
    "        prec = precision_score(backtest_res[\"y_true\"], backtest_res[\"y_pred\"])\n",
    "        rec = recall_score(backtest_res[\"y_true\"], backtest_res[\"y_pred\"])\n",
    "        f1 = f1_score(backtest_res[\"y_true\"], backtest_res[\"y_pred\"])\n",
    "        auc = roc_auc_score(backtest_res[\"y_true\"], backtest_res[\"prob\"])\n",
    "        results_summary[horizon] = {\"precision\": prec, \"recall\": rec, \"f1\": f1, \"auc\": auc}\n",
    "        print(f\"Backtest metrics for {horizon}: precision={prec:.3f}, recall={rec:.3f}, f1={f1:.3f}, auc={auc:.3f}\")\n",
    "        # Train final model on all data (except the very tail where labels missing)\n",
    "        final_train = df.dropna(subset=[label_col])\n",
    "        final_model = train_single_model(final_train[feature_cols], final_train[label_col], model_type=\"xgb\", n_iter=30)\n",
    "        save_model(final_model, f\"model_{horizon}\")\n",
    "        print(\"Saved final model for\", horizon)\n",
    "    # Step 4: Train crash risk model\n",
    "    print(\"Training crash risk model...\")\n",
    "    crash_label = \"crash_very_short\"  # you can choose horizon for crash prediction\n",
    "    # build dataset for crash label - for multi-horizon you can train separate crash models as well\n",
    "    df_crash = df.dropna(subset=[crash_label])\n",
    "    features = feature_cols\n",
    "    if len(df_crash) > 500:\n",
    "        risk_model = train_single_model(df_crash[features], df_crash[crash_label], model_type=\"lgb\", n_iter=30)\n",
    "        save_model(risk_model, \"model_crash\")\n",
    "        print(\"Saved crash risk model.\")\n",
    "    else:\n",
    "        print(\"Not enough samples to train reliable crash model. Need more data points.\")\n",
    "    print(\"All done. Models saved in\", MODEL_DIR)\n",
    "    print(\"Summary:\", results_summary)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
